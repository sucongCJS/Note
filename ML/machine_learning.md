# 基础概念
## 数据
数据整体叫数据集(data set)
每一行数据称为一个样本(sample)
除最后一列, 每一列表达样本的一个特征(feature)
最后一列, 称为标记(label)

## 主要任务
### 分类任务
分类任务的本质就是在特征空间切分
- 多分类
  一些算法只支持二分类任务
  多分类任务可以转换成二分类任务
- 多标签分类
  将一个东西分到多个类中

### 回归任务
结果是一个连续数字的值, 而非一个类别
回归任务可以简化为分类任务

## 机器学习的分类
### 监督学习

> supervised learning 

### 非监督学习

> unsupervised learning 

很多时候非监督学习可以辅助监督学习
可以对没有"标记"的数据进行分类-聚类分析
对数据进行降维处理
  特征提取: 提取有用的特征
  特征压缩: PCA 尽量少的损失前提下压缩
降维处理的意义: 方便可视化
异常检测

### 半监督学习
一部分数据有"标记", 一部分数据没有

通常是先使用无监督学习手段对数据进行处理, 之后使用监督学习手段做模型的训练和预测
### 增强学习
> 强化学习



## 机器学习的其他分类
### 批量学习
### 在线学习
### 参数学习
一旦学到参数, 就不再需要原有的数据集
### 非参数学习
非参数不等于没参数

# 线性回归
均方误差 MSE
> mean-sqare error
估计值-真值 平方和后平均

# 逻辑回归
逻辑回归既可以看做是回归算法, 也可以看作是分类算法(通过概率来分类)
通常作为分类算法用, 可以解决二分类问题

用sigmoid函数将线性回归中原来值域是(-inf, +inf), 转换为(0,1)

## 损失函数
if y=1, cost = -log(^p) # y表示真实的分类, ^p表示估计值, cost表示损失
if y=0, cost = -log(1-^p)
合并成:
cost = -ylog(^p) - (1-y)log(1-^p)

