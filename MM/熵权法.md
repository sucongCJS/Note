# 原理

指标的变异程度(方差)越小, 所反映的信息量也越少, 其对应的权值也应该越低. (比较客观)

一个极端的例子: 一个指标对每个样本的数值一样, 那么这个指标的权值为0, 这个指标没有用

越可能发生的事情, 信息量越少; 越不可能发生的事情, 信息量就越大

随机变量的信息熵越大, 则它的值(内容)能给你补充的信息量越大, 而知道这个值之前你已有的信息量越小. 二者并不矛盾, 前面是针对未来的, 后面是针对现在的, 而熵权法针对的是现在. 

# 信息效用值

$e_j$越大, 即第$j$个指标的信息熵越大, 表面第$j$个指标的信息越少



当$p(x_1) = p(x_2) = ...=p(x_n) = \frac{1}{n}$时, $H(x)$取最大值, 此时$H(x) = lnn$



